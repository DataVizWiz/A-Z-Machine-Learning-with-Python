{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "891a938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e7dc2",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8481033",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc3e809",
   "metadata": {},
   "source": [
    "First we are going to apply some transformation on our training set only. This is to avoid overfitting. These transformations are going to be geometric transformations such as some zooms or rotations.\n",
    "\n",
    "The technical term is image augmentation which is the process of transforming the training data so your CNN model doesn't over learn (over train) on the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ce728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255, \n",
    "    shear_range=0.2, \n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    'dataset/training_set',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b183f",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d3cca99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    'dataset/test_set',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42be7dc0",
   "metadata": {},
   "source": [
    "## Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cc0422",
   "metadata": {},
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9477cccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 08:31:07.630430: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fed837e",
   "metadata": {},
   "source": [
    "### Step 1: Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35f2a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(\n",
    "    filters=32, \n",
    "    kernel_size=3, \n",
    "    activation='relu', \n",
    "    input_shape=[64, 64, 3]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ca3fa3",
   "metadata": {},
   "source": [
    "### Step 2: Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85ddff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(\n",
    "    pool_size=(2, 2), \n",
    "    strides=2\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b2c6e",
   "metadata": {},
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae9f8b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(\n",
    "    filters=32, \n",
    "    kernel_size=3, \n",
    "    activation='relu', \n",
    "    input_shape=[64, 64, 3]\n",
    "))\n",
    "\n",
    "cnn.add(tf.keras.layers.MaxPool2D(\n",
    "    pool_size=(2, 2), \n",
    "    strides=2\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07295d9c",
   "metadata": {},
   "source": [
    "### Step 3: Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd7ebce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c4694d",
   "metadata": {},
   "source": [
    "### Step 4: Full connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a17e11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(\n",
    "    units=128, \n",
    "    activation='relu'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b42a9",
   "metadata": {},
   "source": [
    "### Step 5: Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe929a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(\n",
    "    units=1,\n",
    "    activation='sigmoid'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b3bdbe",
   "metadata": {},
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e71c5d1",
   "metadata": {},
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e94d42c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31a86b2",
   "metadata": {},
   "source": [
    "### Training the CNN on the training set and evaluating it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eec4826b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 38s 151ms/step - loss: 0.6799 - accuracy: 0.5592 - val_loss: 0.6305 - val_accuracy: 0.6670\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 0.6077 - accuracy: 0.6736 - val_loss: 0.6033 - val_accuracy: 0.6705\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 31s 124ms/step - loss: 0.5728 - accuracy: 0.6991 - val_loss: 0.5840 - val_accuracy: 0.7085\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 31s 126ms/step - loss: 0.5400 - accuracy: 0.7255 - val_loss: 0.5204 - val_accuracy: 0.7455\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 32s 129ms/step - loss: 0.5222 - accuracy: 0.7398 - val_loss: 0.5032 - val_accuracy: 0.7495\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 30s 119ms/step - loss: 0.5036 - accuracy: 0.7494 - val_loss: 0.5282 - val_accuracy: 0.7425\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 0.4852 - accuracy: 0.7660 - val_loss: 0.4642 - val_accuracy: 0.7860\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 32s 126ms/step - loss: 0.4635 - accuracy: 0.7810 - val_loss: 0.4814 - val_accuracy: 0.7750\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 31s 124ms/step - loss: 0.4475 - accuracy: 0.7889 - val_loss: 0.4953 - val_accuracy: 0.7830\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 32s 129ms/step - loss: 0.4435 - accuracy: 0.7847 - val_loss: 0.5146 - val_accuracy: 0.7745\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 31s 125ms/step - loss: 0.4287 - accuracy: 0.8010 - val_loss: 0.4644 - val_accuracy: 0.7925\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 31s 122ms/step - loss: 0.4057 - accuracy: 0.8106 - val_loss: 0.4637 - val_accuracy: 0.7950\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 30s 119ms/step - loss: 0.4048 - accuracy: 0.8115 - val_loss: 0.4617 - val_accuracy: 0.7840\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 31s 124ms/step - loss: 0.3923 - accuracy: 0.8202 - val_loss: 0.4564 - val_accuracy: 0.7975\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.3781 - accuracy: 0.8280 - val_loss: 0.4789 - val_accuracy: 0.8055\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 35s 142ms/step - loss: 0.3662 - accuracy: 0.8381 - val_loss: 0.4514 - val_accuracy: 0.8050\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 0.3564 - accuracy: 0.8414 - val_loss: 0.4721 - val_accuracy: 0.7920\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 37s 147ms/step - loss: 0.3407 - accuracy: 0.8505 - val_loss: 0.5177 - val_accuracy: 0.7700\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 33s 133ms/step - loss: 0.3352 - accuracy: 0.8476 - val_loss: 0.5076 - val_accuracy: 0.7855\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 0.3209 - accuracy: 0.8560 - val_loss: 0.4849 - val_accuracy: 0.7940\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 0.3200 - accuracy: 0.8621 - val_loss: 0.5147 - val_accuracy: 0.7925\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 35s 139ms/step - loss: 0.3073 - accuracy: 0.8664 - val_loss: 0.4823 - val_accuracy: 0.8045\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 35s 140ms/step - loss: 0.2946 - accuracy: 0.8749 - val_loss: 0.5033 - val_accuracy: 0.8080\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 35s 138ms/step - loss: 0.2910 - accuracy: 0.8751 - val_loss: 0.4923 - val_accuracy: 0.8100\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 0.2658 - accuracy: 0.8850 - val_loss: 0.5565 - val_accuracy: 0.8065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8f67cce590>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=training_set, validation_data=test_set, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404b7e8a",
   "metadata": {},
   "source": [
    "## Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2664a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size=(64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "\n",
    "if (result[0][0] == 1):\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9102590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20239906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_2.jpg', target_size=(64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "\n",
    "if (result[0][0] == 1):\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6bda519c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9622f7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_3.jpg', target_size=(64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "\n",
    "if (result[0][0] == 1):\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2360238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06b673c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_4.jpg', target_size=(64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "\n",
    "if (result[0][0] == 1):\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32d05289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f055a",
   "metadata": {},
   "source": [
    "We can see that when we tried to predict image 4, which was a cat, we ended up with dog. This was a challenging photo as it was only a cats head so some features the model would depend on would be unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d50f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
